{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence for Beginners\n",
    "## Introduction\n",
    "![](https://raw.githubusercontent.com/GNS-Science/AI_workshop_GNS_staff_conference/christof/notebook_images/AI_ML_DL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification\n",
    "![](https://raw.githubusercontent.com/GNS-Science/AI_workshop_GNS_staff_conference/christof/notebook_images/CNN_scheme.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today's Exercise\n",
    "\n",
    "Image classification of 3 types of Fossil with a Deep  Convolutional Neural Network\n",
    "\n",
    "1. Design our own model\n",
    "2. Tune the architecture of a modelÂ \n",
    "3. Use a pre-trained model and transfer learning\n",
    "\n",
    "![](https://raw.githubusercontent.com/GNS-Science/AI_workshop_GNS_staff_conference/christof/notebook_images/mf_composite.png)\n",
    "\n",
    "To follow along follow this link: **http://bit.ly/3JsfxV0** or scan the QR code\n",
    "\n",
    "![](https://raw.githubusercontent.com/GNS-Science/AI_workshop_GNS_staff_conference/christof/notebook_images/QR_code.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAOA3X-_IPY0"
   },
   "source": [
    "## Fossil Classification with Deep Learning\n",
    "* The micro-fossil data is provided by the Paleontology team, GNS Science.\n",
    "* We will train and evaluate a deep convolutional neural network (CNN) to classify images of fossils\n",
    "* To speed up computation we have selected 3 out of 14 different classes\n",
    "\n",
    "### 1. Simple Convolutional Neural Network\n",
    "\n",
    "First, we will:\n",
    "1. Create a simple CNN\n",
    "2. Train it on a subset of the data (the training dataset)\n",
    "3. Evaluate its performance on the rest of the dataset (the testing dataset)\n",
    "\n",
    "**Note**: `nn_utils.py` is a Python script with some helper functions to keep this demo brief. Have a look at it by double-clicking on it in the files tab.\n",
    "\n",
    "**Let's start!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install [tensorflow](https://www.tensorflow.org/) to code the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ek6VH1rHOlUj"
   },
   "outputs": [],
   "source": [
    "# #specify the tensorflow version\n",
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow import keras\n",
    "!pip install -q -U keras-tuner gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBYiMfGifHkc"
   },
   "source": [
    "#### Get the training data and the Python script with the helper functions from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNQC_N2fQtHR"
   },
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/GNS-Science/AI_workshop_GNS_staff_conference/main/ImageClassesM.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bDM3YvjZ93_m"
   },
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/GNS-Science/AI_workshop_GNS_staff_conference/main/nn_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTu9s2LEuVTm"
   },
   "source": [
    "#### Unzip the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLOFbyBifAo8"
   },
   "outputs": [],
   "source": [
    "!unzip ImageClassesM.zip > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IeGD-zI_mu_E"
   },
   "outputs": [],
   "source": [
    "# You can also upload files directly from your computer\n",
    "# by using the following code\n",
    "# from google.colab import files\n",
    "# uploaded =  files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the helper functions module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kOrsRSW1LPTH"
   },
   "outputs": [],
   "source": [
    "import nn_utils\n",
    "# -- function to split a dataset into a training and a testing dataset\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YyFO2BxFmfSI"
   },
   "source": [
    "#### Import the images and store them into a `Numpy` array along a class array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QutCOznWmx-3"
   },
   "outputs": [],
   "source": [
    "# -- read data, format images into an Array and encode the classes\n",
    "input_dir = \"/content/ImageClassesM\"\n",
    "ImageArray, Labels = nn_utils.read_images(images_dir=input_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fqihkMhq2pm"
   },
   "source": [
    "#### Split the dataset into *training* and *testing*.\n",
    "\n",
    "This dataset like many other is not balanced, i.e. there are more images of certain classes than others. If not taken into account this could lead to a biased model. When spliting the dataset, we will use *Stratified Sampling* to ensure that each class within the dataset receives a statistically accurate representation within both training and testing subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "54iuYnWMq-rk"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(ImageArray, Labels, test_size=0.2, random_state=42, stratify=Labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_w0FWD4sZOG"
   },
   "source": [
    "#### Create our simple CNN model and use the training dataset to teach the model to recognize fossils.\n",
    "\n",
    "During training, it is common to keep a small portion of the training dataset to *validate* or test the model as it is training. This always ensures that the model's accuracy is tested on never-seen-before data. The `batch_size` controls the number of images (in this case) fed to the model at a time and the number of `epochs` is the number of times it is trained on the training dataset. We will also plot a summary of the training to see how it went."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4BKc6kc9sX9W"
   },
   "outputs": [],
   "source": [
    "image_shape = ImageArray.shape[1:] # dimension of one image -> input\n",
    "number_of_class = y_train.shape[1] # number of class, size of the prediction distribution -> output\n",
    "\n",
    "model, summary = nn_utils.CNNbase_model(\n",
    "        input_shape=image_shape, n_class=number_of_class)\n",
    "\n",
    "# -- training and evaluating the model with a subset of unseen data\n",
    "history = model.fit(\n",
    "    x=X_train, y=y_train, batch_size=32, validation_split=0.2, epochs=25)\n",
    "\n",
    "nn_utils.summarize_diagnostics(history=history, filename=\"/content/base_summary.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZSD97zlgjkT"
   },
   "outputs": [],
   "source": [
    "eval_result = model.evaluate(x=X_test, y=y_test)\n",
    "print(f\"\"\"\n",
    "        Evaluation of test data [test loss, test accuracy] is {eval_result}.\n",
    "       \"\"\")\n",
    "\n",
    "# -- try on some examples\n",
    "X_sample, y_sample = nn_utils.sample_dataset(X=X_test, y=y_test, size=9, random_state=42)\n",
    "y_pred = model.predict(x=X_sample)\n",
    "nn_utils.plot_prediction(x=X_sample, y_true=y_sample, y_pred=y_pred, filename=\"/content/base_prediction.jpg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-kEjs0Jqpy3"
   },
   "source": [
    "Often the first model (CNN) is not performing well. This could be due to multiple factors, e.g. the training dataset is too small, the architecture of the model is too simple, etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pepckmTEhbQH"
   },
   "source": [
    "### 2. Model optimization and hyperparameters tunning\n",
    "\n",
    "We know that one of the limitation of the previous model is its architecture. Optimizing the architecture and tunning the hyperparameters (*hypertuning*) of a Machine Learning model is almost an art by itself. You can find more details on `Tensorflow`'s tuner [here!](https://www.tensorflow.org/tutorials/keras/keras_tuner).\n",
    "\n",
    "Here, the parameters that will be optimized are for example the size of the window to perform the 2D convolution in the first half of the model or the number of hidden layers in the second half of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cgct2Q2rqnM4"
   },
   "outputs": [],
   "source": [
    "image_shape = ImageArray.shape[1:] # dimension of one image -> input\n",
    "number_of_class = y_train.shape[1] # number of class, size of the prediction distribution -> output\n",
    "\n",
    "tuner = nn_utils.hyper_tuning(input_shape=image_shape, n_class=number_of_class)\n",
    "tuner.search(X_train, y_train, validation_split=0.2)\n",
    "tuner.results_summary()\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(1)[0]\n",
    "print(f\"\"\"\n",
    "        The hyperparameter search is complete. The optimal number of\n",
    "        layers {best_hps.get('num_layers')} and the optimal learning rate for the optimizer\n",
    "        is {best_hps.get('learning_rate')}. Best Conv. Filter: {best_hps.get('conv_filter')}. \n",
    "        Best Conv. Kernel: {best_hps.get('conv_kernel')}. \n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6bE09Ycsq0D"
   },
   "source": [
    "Once the tuner is done, we get a set of hyperparameters that correspond to the best results on the validation dataset, which is here 20% of the training dataset. We can then train a model with these hyperparameters and evaluate our new model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mrVHOkvysual"
   },
   "outputs": [],
   "source": [
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "history = best_model.fit(\n",
    "    x=X_train, y=y_train, batch_size=32, validation_split=0.2, epochs=10)\n",
    "\n",
    "# -- 3.3 testing the model\n",
    "eval_result = best_model.evaluate(x=X_test, y=y_test)\n",
    "print(f\"\"\"\n",
    "            Evaluation of test data [test loss, test accuracy] is {eval_result}.\n",
    "        \"\"\")\n",
    "\n",
    "# -- plot summary \n",
    "nn_utils.summarize_diagnostics(history=history, filename=\"/content/tuned_summary.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fghOc9xGtka8"
   },
   "source": [
    "Let's look at some examples.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_C12mxxtmzf"
   },
   "outputs": [],
   "source": [
    "X_sample, y_sample = nn_utils.sample_dataset(X=X_test, y=y_test, size=9, random_state=42)\n",
    "y_pred = best_model.predict(x=X_sample)\n",
    "nn_utils.plot_prediction(x=X_sample, y_true=y_sample, y_pred=y_pred, filename=\"/content/tuned_prediction.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9fIiXWSt10b"
   },
   "source": [
    "\n",
    "Still not very convincing, right? Remember it might not be **just** the architecture, most likely our training dataset is too limited. We will see in the next exercise how to use a pre-trained model and transfer learning to have a highly accurate model to classify our images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2KEAspJt7LF"
   },
   "source": [
    "### 3. Transfer learning and how to use a pre-trained model\n",
    "\n",
    "One other limitation of today's exercises is that we are limited by the number of fossil images. Fortunately, a lot of people have trained many other models for image classification. While there might not be a specific model trained to classify fossil images we can still make use of a model trained on other types of images. We are going to use the **ResNet50** model, a CNN 50-layers deep trained on more than a million labelled images such as ballons or strawberries from https://www.image-net.org/. To be able to use it for classifying fossils, we are going to use a principle called *Transfer Learning*: we will use a model trained on some data and apply it on some other data. \n",
    "\n",
    "In practice, this consists of *replacing* the last part of the CNN, the fully connected layer which does the classification based on the features extracted by the convolutional layers, by a new fully connected layer that needs to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHYz1Gcht9hO"
   },
   "outputs": [],
   "source": [
    "# -- 3.1 init of the Model with new top layer\n",
    "image_shape = ImageArray.shape[1:] # dimension of one image -> input\n",
    "number_of_class = y_train.shape[1] # number of class, size of the prediction distribution -> output\n",
    "model = nn_utils.CNNtrained_model(input_shape=image_shape, n_class=number_of_class)\n",
    "\n",
    "# -- 3.2 training and evaluating the model with a subset of unseen data (validation)\n",
    "history = model.fit(x=X_train, y=y_train, batch_size=32, validation_split=0.2, epochs=15)\n",
    "\n",
    "# -- 3.3 testing the model\n",
    "eval_result = model.evaluate(x=X_test, y=y_test)\n",
    "print(f\"\"\"\n",
    "            Evaluation of test data [test loss, test accuracy] is {eval_result}.\n",
    "        \"\"\")\n",
    "\n",
    "# -- 3.4 plot summary \n",
    "nn_utils.summarize_diagnostics(history=history, filename=\"/content/pretrained_summary.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64G8ozhuuO6N"
   },
   "source": [
    "It's a win! By using Transfer Learning, we clearly see an improvement. Let's have a look at some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oyheUYeftsfP"
   },
   "outputs": [],
   "source": [
    "# -- 3.5 Show some examples\n",
    "X_sample, y_sample = nn_utils.sample_dataset(X=X_test, y=y_test, size=9, random_state=42)\n",
    "y_pred = model.predict(x=X_sample)\n",
    "nn_utils.plot_prediction(x=X_sample, y_true=y_sample, y_pred=y_pred, filename=\"/content/pretrained_prediction.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tFPtxyKHvQN"
   },
   "source": [
    "Not a single mistake! \n",
    "This new model to classify images of fossils is much more accurate than our first models thanks to the **ResNet50** pre-trained model. In practice, we benefit from the already very well trained filters that are in the convolutional layers to extract meaningful and accurate features. In fact, Transfer Learning is becoming common: [a good example](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2021JB021910?utm_sq=gqzx3u0l8q) is a model trained to pick seismic wave arrivals in Southern California which was used to create a model specifically trained for picking events at Nabro Volcano, a Stratovolcano in Eritrea. \n",
    "\n",
    "### Conclusion\n",
    "\n",
    "* Colab allows us to get started with Python and Machine Learning without having to install anything\n",
    "* Setting up a machine learning model is like combining Lego bricks thanks to many available Python packages\n",
    "* Deep Learning is basically multi-parameter function-fitting (non-linear functions with many parameters)\n",
    "* To avoid over-fitting we use cross-validation\n",
    "* Transfer learning is a great option when the datasets are small\n",
    "* Training Deep Learning models from scratch requires a lot of data\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "609d640725c18646c22904024d298faf262d85278ca8cfe22ae2a8de8a5c058b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
